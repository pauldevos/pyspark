{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "- Driver and Worker Node(s)\n",
    "- Code is composed of `Transformations` and `Actions`\n",
    "- The lineage of `transformations` are collected in the form of a __directed acyclic graph (DAG)__ which are then executed when an `Action` is called by the driver node. With the help of the DAG, Spark can make key optimizations to reduce the time of execution and improve the performance of the cluster of nodes.\n",
    "\n",
    "### Performance things to watch\n",
    "- Reduce I/O between Nodes\n",
    "- Monitor/manage data in memory\n",
    "\n",
    "### RDD\n",
    "- Lowest level API in Spark\n",
    "- Strengths:\n",
    "    - Fault Tolerant\n",
    "        - Utilizes a restricted form of sharded memory between a cluster of nodes\n",
    "        - Forming a coarse-grained linear of transformations\n",
    "        - As opposed to relying on fine-grained updates\n",
    "        \n",
    "#### When to use RDDs?\n",
    "- When you are working with operations that `need conversion at a `__`row/record level`__ for special data structures ___(text or media files)___.\n",
    "- When the _flexibility_ in __modifying data at a granular level__ is more important.\n",
    "- When `schema` becomes _irrelevant_ to your use case but __parallelization__ can help.\n",
    "- When you are not going to use the __domain-specific expressions__ (think of __Spark SQL__ abstractions)\n",
    "\n",
    "#### When to avoid RDDs?\n",
    "- When you want a schema\n",
    "- When you want to use the Spark catalyst optimizations along with the column name access.\n",
    "- When you want to avoid complex coding constructs for simple API operations, e.g. finding the average frequency of words in a file. (Basically when semi-unstructured or unstructured data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "- It is a higher-level abstraction from RDDs and is powered by a schema that also allows Spark to perform more automated optimizations at runtime using the __Catalyst optimizer__.\n",
    "- When joining two RDDs converting them into dataframes can be beneficial due to the presence of the __Catalyst optimizer__. It speeds up the execution process using the power of quasi-quotes and pattern matching in Scala.\n",
    "\n",
    "\n",
    "#### Dataframes can be created using the following ways:\n",
    "- from RDDs using the `inferSchema` option (or) using a `custom schema`.\n",
    "- from files that are in different formats (`JSON, Parquet, CSV, Avro` etc.).\n",
    "- from datasets using the implicit conversion `toDF` method.\n",
    "\n",
    "Untyped nature of Dataframes - Type Checking at run time\n",
    "\n",
    "DataFrames are a set of are generic Row objects which hold the data and they do have types. The word untyped references the time at which the type-checking is done with a Dataframe. It is ___done only during `run time`___ based on the schema that was inferred or defined by the user.\n",
    "\n",
    "Shortcomings of the Dataframe\n",
    "\n",
    "It was not able to use UDFs efficiently with optimization.\n",
    "There is a lack of strong typing that can be achieved in Scala/Java.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSets\n",
    "\n",
    "#### Features of Datasets\n",
    "- Seamless support for semi-structured data.\n",
    "- Compile-time type-safety (Syntax + Analysis errors).\n",
    "- Encoders increased the serialization/deserialization speed.\n",
    "- Single API for Java & Scala.\n",
    "\n",
    "\n",
    "**Datasets** were introduced in Spark release 1.6.0 (early 2016). It brought the advantage of **strong type checking** at ___compile time___ itself.\n",
    "\n",
    "The fundamental concept of bringing in type safety was via the introduction of **Encoders** that can convert a `JVM object` of `type T` into an `internal binary representation`. It is also a serialization & deserialization (__SerDe__) framework. Encoders represent the schema of records which avoids the unnecessary conversions of JVM objects. It enforces a mapping from a domain object to the internal binary representation. They provide super fast conversions compared to __Java or Kryo serialization__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification of the Dataset and DataFrame APIs\n",
    "\n",
    "Dataset & Dataframe were separate APIs until eventually two of the musketeers combined to form the Unified Dataset API in the Spark 2.0.0 release (late 2016).\n",
    "\n",
    "This unification coalesced the advantages of RDDs and Dataframe APIs into one umbrella.\n",
    "\n",
    "- Dataframe became a type alias of `Dataset[Row]`. \n",
    "- In terms of languages, the Dataframe remained to be the primary abstraction in Python & R languages as they are analogous to the `single-node dataframe`.\n",
    "- In Scala & Java, **datasets** represented the `typed version` of the API and the **dataframe** is the `untyped version`.\n",
    "\n",
    "\n",
    "#### When to use Unified Dataset [Dataframe / Dataset] API?\n",
    "- When we are planning to use high-level abstractions on domain specific abstractions (aggregations, joins etc.) with schema enforcement.\n",
    "- Columnar access, lambda functions on semi-structured data.\n",
    "- For a higher degree of type-safety check at compile-time, we can use the typed version of the unified dataset API.\n",
    "- To benefit from the tungsten code generation (faster expression evaluation using dataframe/SQL operators).\n",
    "- R users are recommended to use Dataframes (Dataset is not available).\n",
    "- Python users can also use Dataframes (Dataset is not available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "- pros: has optimized operations over column oriented storages\n",
    "- pros: also many operations doesn't need deserialization\n",
    "- pros: provide table/sql semantic if you like it (I don't ;)\n",
    "- pros: dataset operations comes with an optimization engine \"catalyst\" that improves the performance of your code (I'm not sure if it is really that great. If you know what you code, meaning what is done to the data, your code should be optimized by itself)\n",
    "- cons: most operation loose typing\n",
    "- cons: dataset operations can become too complicated for complex algorithm that doesn't suit it. The 2 main limits I know are managing invalid data and complex math algorithm.\n",
    "\n",
    "\n",
    "#### Dataframe:\n",
    "\n",
    "- pros: required between dataset operations that lose type\n",
    "- cons: just use Dataset it has all the advantages and more\n",
    "\n",
    "\n",
    "#### RDD:\n",
    "- pros: (really) strongly typed\n",
    "- pros: scala/java semantic. You can design your code pretty much how you would for an standard app that process in-memory collections. Well, with functional semantic :)\n",
    "- cons: full jvm deserialization is required to process data, at any step mentioned before: after reading input, and between all processing steps that requires data to be moved between worker, or stored locally to manage memory bound.\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Just use **Dataset** by default:\n",
    "\n",
    "- read input with an Encoder, if the data format allows it it will validate input schema at start\n",
    "- use dataset operations and when you loose type, go back to a typed dataset.\n",
    "- Typically, use typed dataset as input and output of all methods.\n",
    "\n",
    "There are cases where what you want to code would be too complex to express using dataset operations. Most app doesn't, but it often happen in my work where I implements complex mathematical models. In this case:\n",
    "- start with dataset\n",
    "- filter and shuffle (groupBy, join) data as much as possible with dataset op\n",
    "- once you have only the required data, and need not move them, convert to rdd and apply you complex computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 09:36:36 WARN Utils: Your hostname, Pauls-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.90 instead (on interface en0)\n",
      "22/06/06 09:36:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/06 09:36:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.90:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_assert_on_driver',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getCheckpointDir',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_jwrapped',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_wrapped',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or create a dataframe using the `schema` parameter\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a PySpark Dataframe from a Pandas DataFrame\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a PySpark Dataframe from a RDD which is composed of a list of tuples\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All DataFrames above result same.\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "-RECORD 2------------------\n",
      " a   | 3                   \n",
      " b   | 4.0                 \n",
      " c   | string3             \n",
      " d   | 2000-03-01          \n",
      " e   | 2000-01-03 12:00:00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+-------+\n",
      "|summary|  a|  b|      c|\n",
      "+-------+---+---+-------+\n",
      "|  count|  3|  3|      3|\n",
      "|   mean|2.0|3.0|   null|\n",
      "| stddev|1.0|1.0|   null|\n",
      "|    min|  1|2.0|string1|\n",
      "|    max|  3|4.0|string3|\n",
      "+-------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataFrame.collect()`** collects the distributed data to the `driver side` as the local data in Python. Note that this can throw an **out-of-memory error** when the dataset is too large to fit in the `driver side` because it collects all the data from executors to the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`PySpark DataFrame`** also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that `toPandas` ___also collects all data___ into the **driver side** that can easily cause an `out-of-memory-error` when the data is too large to fit into the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance.\n",
    "\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                  e|\n",
      "+-------------------+\n",
      "|2000-01-01 12:00:00|\n",
      "|2000-01-02 12:00:00|\n",
      "|2000-01-03 12:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These Columns can be used to select the columns from a DataFrame. \n",
    "# For example, DataFrame.select() takes the Column instances that returns another DataFrame.\n",
    "\n",
    "df.select(df.e).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to add a new column of data\n",
    "df.withColumn('upper_c', upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a subset of rows, use **`DataFrame.filter()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.b > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(a)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|                 3|\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying a function\n",
    "import pandas\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 11:51:12 ERROR Executor: Exception in task 2.0 in stage 59.0 (TID 210)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 915, in pyarrow.lib.Array.from_pandas\n",
      "  File \"pyarrow/array.pxi\", line 312, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n",
      "  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowTypeError: Expected a string or bytes dtype, got int64\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/06 11:51:12 WARN TaskSetManager: Lost task 2.0 in stage 59.0 (TID 210) (192.168.4.90 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 915, in pyarrow.lib.Array.from_pandas\n",
      "  File \"pyarrow/array.pxi\", line 312, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n",
      "  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowTypeError: Expected a string or bytes dtype, got int64\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/06/06 11:51:12 ERROR TaskSetManager: Task 2 in stage 59.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"pyarrow/array.pxi\", line 915, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow/array.pxi\", line 312, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.check_status\npyarrow.lib.ArrowTypeError: Expected a string or bytes dtype, got int64\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@pandas_udf\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpandas_plus_one\u001b[39m(series: pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Simply plus one by using pandas Series.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_plus_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"pyarrow/array.pxi\", line 915, in pyarrow.lib.Array.from_pandas\n  File \"pyarrow/array.pxi\", line 312, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.check_status\npyarrow.lib.ArrowTypeError: Expected a string or bytes dtype, got int64\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('string') # note this was a string, and the function got a series of numbers\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(a,LongType,true),StructField(b,DoubleType,true),StructField(c,StringType,true),StructField(d,DateType,true),StructField(e,TimestampType,true)))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(b)|\n",
      "+------------------+\n",
      "|                 3|\n",
      "|                 4|\n",
      "|                 5|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "\n",
    "# df.b is a double, but works with a long\n",
    "df.select(pandas_plus_one(df.b)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(a)|\n",
      "+------------------+\n",
      "|               2.0|\n",
      "|               3.0|\n",
      "|               4.0|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('double')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "\n",
    "# df.a is a long, but works with a double\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|sum(v1)|sum(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|     24|    240|\n",
      "| blue|      6|     60|\n",
      "|black|      6|     60|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"color\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------+-------+\n",
      "|color|count(color)|sum(v1)|avg(v2)|\n",
      "+-----+------------+-------+-------+\n",
      "|  red|           5|     24|   48.0|\n",
      "| blue|           2|      6|   30.0|\n",
      "|black|           1|      6|   60.0|\n",
      "+-----+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"color\").agg({\"color\": 'count', 'v1': \"sum\", 'v2': 'mean'}).show() # can't alias column names tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, sum, stddev, count, column, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+------------------+\n",
      "|color|Count_of_color|v1_avg|         stddev_v1|\n",
      "+-----+--------------+------+------------------+\n",
      "|  red|             5|   4.8|2.8635642126552705|\n",
      "| blue|             2|   3.0|1.4142135623730951|\n",
      "|black|             1|   6.0|              null|\n",
      "+-----+--------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"color\").agg(count('color').alias('Count_of_color'),\n",
    "                       mean(\"v1\").alias(\"v1_avg\"),\n",
    "                       stddev('v1').alias(\"stddev_v1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+--------------+------+---------+-------------+\n",
      "|color| v1| v2|Count_of_color|v1_avg|stddev_v1|product_v1_v2|\n",
      "+-----+---+---+--------------+------+---------+-------------+\n",
      "|  red|  1| 10|             1|   1.0|     null|           10|\n",
      "| blue|  2| 20|             1|   2.0|     null|           40|\n",
      "|  red|  3| 30|             1|   3.0|     null|           90|\n",
      "| blue|  4| 40|             1|   4.0|     null|          160|\n",
      "|  red|  5| 50|             1|   5.0|     null|          250|\n",
      "|black|  6| 60|             1|   6.0|     null|          360|\n",
      "|  red|  7| 70|             1|   7.0|     null|          490|\n",
      "|  red|  8| 80|             1|   8.0|     null|          640|\n",
      "+-----+---+---+--------------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby([\"color\", \"v1\", \"v2\"]).agg(count('color').alias('Count_of_color'),\n",
    "                       mean(\"v1\").alias(\"v1_avg\"),\n",
    "                       stddev('v1').alias(\"stddev_v1\"),\n",
    "                       (column('v1') * column('v2')).alias('product_v1_v2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------+---------+\n",
      "|color|Count_of_color|v1_avg|stddev_v1|\n",
      "+-----+--------------+------+---------+\n",
      "|  red|             5|   4.8|    2.864|\n",
      "| blue|             2|   3.0|    1.414|\n",
      "|black|             1|   6.0|     null|\n",
      "+-----+--------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby([\"color\"]).agg(count('color').alias('Count_of_color'),\n",
    "                       mean(\"v1\").alias(\"v1_avg\"),\n",
    "                       round(stddev('v1'),3).alias(\"stddev_v1\"),).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+\n",
      "|count(color)|sum(v1)|avg(v2)|\n",
      "+------------+-------+-------+\n",
      "|           8|     36|   45.0|\n",
      "+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"color\": 'count', 'v1': \"sum\", 'v2': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "+--------+---+---+\n",
      "|    time| id| v1|\n",
      "+--------+---+---+\n",
      "|20000101|  1|1.0|\n",
      "|20000101|  2|2.0|\n",
      "|20000102|  1|3.0|\n",
      "|20000102|  2|4.0|\n",
      "+--------+---+---+\n",
      "\n",
      "df2\n",
      "+--------+---+---+\n",
      "|    time| id| v2|\n",
      "+--------+---+---+\n",
      "|20000101|  1|  x|\n",
      "|20000101|  2|  y|\n",
      "+--------+---+---+\n",
      "\n",
      "df1 left join df2 nearest match\n",
      "+--------+---+---+---+\n",
      "|    time| id| v1| v2|\n",
      "+--------+---+---+---+\n",
      "|20000101|  1|1.0|  x|\n",
      "|20000102|  1|3.0|  x|\n",
      "|20000101|  2|2.0|  y|\n",
      "|20000102|  2|4.0|  y|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data=\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    schema=('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on='time', by='id')\n",
    "\n",
    "print(\"df1\")\n",
    "df1.show()\n",
    "print(\"df2\")\n",
    "df2.show()\n",
    "print(\"df1 left join df2 nearest match\")\n",
    "(df1.groupby('id').cogroup(df2.groupby('id'))\n",
    "            .applyInPandas(asof_join, \n",
    "                           schema='time int, id int, v1 double, v2 string').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function merge_asof in module pandas.core.reshape.merge:\n",
      "\n",
      "merge_asof(left: 'DataFrame | Series', right: 'DataFrame | Series', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, by=None, left_by=None, right_by=None, suffixes: 'Suffixes' = ('_x', '_y'), tolerance=None, allow_exact_matches: 'bool' = True, direction: 'str' = 'backward') -> 'DataFrame'\n",
      "    Perform a merge by key distance.\n",
      "    \n",
      "    This is similar to a left-join except that we match on nearest\n",
      "    key rather than equal keys. Both DataFrames must be sorted by the key.\n",
      "    \n",
      "    For each row in the left DataFrame:\n",
      "    \n",
      "      - A \"backward\" search selects the last row in the right DataFrame whose\n",
      "        'on' key is less than or equal to the left's key.\n",
      "    \n",
      "      - A \"forward\" search selects the first row in the right DataFrame whose\n",
      "        'on' key is greater than or equal to the left's key.\n",
      "    \n",
      "      - A \"nearest\" search selects the row in the right DataFrame whose 'on'\n",
      "        key is closest in absolute distance to the left's key.\n",
      "    \n",
      "    The default is \"backward\" and is compatible in versions below 0.20.0.\n",
      "    The direction parameter was added in version 0.20.0 and introduces\n",
      "    \"forward\" and \"nearest\".\n",
      "    \n",
      "    Optionally match on equivalent keys with 'by' before searching with 'on'.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    left : DataFrame or named Series\n",
      "    right : DataFrame or named Series\n",
      "    on : label\n",
      "        Field name to join on. Must be found in both DataFrames.\n",
      "        The data MUST be ordered. Furthermore this must be a numeric column,\n",
      "        such as datetimelike, integer, or float. On or left_on/right_on\n",
      "        must be given.\n",
      "    left_on : label\n",
      "        Field name to join on in left DataFrame.\n",
      "    right_on : label\n",
      "        Field name to join on in right DataFrame.\n",
      "    left_index : bool\n",
      "        Use the index of the left DataFrame as the join key.\n",
      "    right_index : bool\n",
      "        Use the index of the right DataFrame as the join key.\n",
      "    by : column name or list of column names\n",
      "        Match on these columns before performing merge operation.\n",
      "    left_by : column name\n",
      "        Field names to match on in the left DataFrame.\n",
      "    right_by : column name\n",
      "        Field names to match on in the right DataFrame.\n",
      "    suffixes : 2-length sequence (tuple, list, ...)\n",
      "        Suffix to apply to overlapping column names in the left and right\n",
      "        side, respectively.\n",
      "    tolerance : int or Timedelta, optional, default None\n",
      "        Select asof tolerance within this range; must be compatible\n",
      "        with the merge index.\n",
      "    allow_exact_matches : bool, default True\n",
      "    \n",
      "        - If True, allow matching with the same 'on' value\n",
      "          (i.e. less-than-or-equal-to / greater-than-or-equal-to)\n",
      "        - If False, don't match the same 'on' value\n",
      "          (i.e., strictly less-than / strictly greater-than).\n",
      "    \n",
      "    direction : 'backward' (default), 'forward', or 'nearest'\n",
      "        Whether to search for prior, subsequent, or closest matches.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    merged : DataFrame\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    merge : Merge with a database-style join.\n",
      "    merge_ordered : Merge with optional filling/interpolation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n",
      "    >>> left\n",
      "        a left_val\n",
      "    0   1        a\n",
      "    1   5        b\n",
      "    2  10        c\n",
      "    \n",
      "    >>> right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n",
      "    >>> right\n",
      "       a  right_val\n",
      "    0  1          1\n",
      "    1  2          2\n",
      "    2  3          3\n",
      "    3  6          6\n",
      "    4  7          7\n",
      "    \n",
      "    >>> pd.merge_asof(left, right, on=\"a\")\n",
      "        a left_val  right_val\n",
      "    0   1        a          1\n",
      "    1   5        b          3\n",
      "    2  10        c          7\n",
      "    \n",
      "    >>> pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n",
      "        a left_val  right_val\n",
      "    0   1        a        NaN\n",
      "    1   5        b        3.0\n",
      "    2  10        c        7.0\n",
      "    \n",
      "    >>> pd.merge_asof(left, right, on=\"a\", direction=\"forward\")\n",
      "        a left_val  right_val\n",
      "    0   1        a        1.0\n",
      "    1   5        b        6.0\n",
      "    2  10        c        NaN\n",
      "    \n",
      "    >>> pd.merge_asof(left, right, on=\"a\", direction=\"nearest\")\n",
      "        a left_val  right_val\n",
      "    0   1        a          1\n",
      "    1   5        b          6\n",
      "    2  10        c          7\n",
      "    \n",
      "    We can use indexed DataFrames as well.\n",
      "    \n",
      "    >>> left = pd.DataFrame({\"left_val\": [\"a\", \"b\", \"c\"]}, index=[1, 5, 10])\n",
      "    >>> left\n",
      "       left_val\n",
      "    1         a\n",
      "    5         b\n",
      "    10        c\n",
      "    \n",
      "    >>> right = pd.DataFrame({\"right_val\": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n",
      "    >>> right\n",
      "       right_val\n",
      "    1          1\n",
      "    2          2\n",
      "    3          3\n",
      "    6          6\n",
      "    7          7\n",
      "    \n",
      "    >>> pd.merge_asof(left, right, left_index=True, right_index=True)\n",
      "       left_val  right_val\n",
      "    1         a          1\n",
      "    5         b          3\n",
      "    10        c          7\n",
      "    \n",
      "    Here is a real-world times-series example\n",
      "    \n",
      "    >>> quotes = pd.DataFrame(\n",
      "    ...     {\n",
      "    ...         \"time\": [\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.030\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.041\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.049\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.072\"),\n",
      "    ...             pd.Timestamp(\"2016-05-25 13:30:00.075\")\n",
      "    ...         ],\n",
      "    ...         \"ticker\": [\n",
      "    ...                \"GOOG\",\n",
      "    ...                \"MSFT\",\n",
      "    ...                \"MSFT\",\n",
      "    ...                \"MSFT\",\n",
      "    ...                \"GOOG\",\n",
      "    ...                \"AAPL\",\n",
      "    ...                \"GOOG\",\n",
      "    ...                \"MSFT\"\n",
      "    ...            ],\n",
      "    ...            \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n",
      "    ...            \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n",
      "    ...     }\n",
      "    ... )\n",
      "    >>> quotes\n",
      "                         time ticker     bid     ask\n",
      "    0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n",
      "    1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n",
      "    2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n",
      "    3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n",
      "    4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n",
      "    5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n",
      "    6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n",
      "    7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n",
      "    \n",
      "    >>> trades = pd.DataFrame(\n",
      "    ...        {\n",
      "    ...            \"time\": [\n",
      "    ...                pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
      "    ...                pd.Timestamp(\"2016-05-25 13:30:00.038\"),\n",
      "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
      "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
      "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\")\n",
      "    ...            ],\n",
      "    ...            \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n",
      "    ...            \"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n",
      "    ...            \"quantity\": [75, 155, 100, 100, 100]\n",
      "    ...        }\n",
      "    ...    )\n",
      "    >>> trades\n",
      "                         time ticker   price  quantity\n",
      "    0 2016-05-25 13:30:00.023   MSFT   51.95        75\n",
      "    1 2016-05-25 13:30:00.038   MSFT   51.95       155\n",
      "    2 2016-05-25 13:30:00.048   GOOG  720.77       100\n",
      "    3 2016-05-25 13:30:00.048   GOOG  720.92       100\n",
      "    4 2016-05-25 13:30:00.048   AAPL   98.00       100\n",
      "    \n",
      "    By default we are taking the asof of the quotes\n",
      "    \n",
      "    >>> pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n",
      "                         time ticker   price  quantity     bid     ask\n",
      "    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n",
      "    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n",
      "    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n",
      "    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n",
      "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
      "    \n",
      "    We only asof within 2ms between the quote time and the trade time\n",
      "    \n",
      "    >>> pd.merge_asof(\n",
      "    ...     trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")\n",
      "    ... )\n",
      "                         time ticker   price  quantity     bid     ask\n",
      "    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n",
      "    1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n",
      "    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n",
      "    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n",
      "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
      "    \n",
      "    We only asof within 10ms between the quote time and the trade time\n",
      "    and we exclude exact matches on time. However *prior* data will\n",
      "    propagate forward\n",
      "    \n",
      "    >>> pd.merge_asof(\n",
      "    ...     trades,\n",
      "    ...     quotes,\n",
      "    ...     on=\"time\",\n",
      "    ...     by=\"ticker\",\n",
      "    ...     tolerance=pd.Timedelta(\"10ms\"),\n",
      "    ...     allow_exact_matches=False\n",
      "    ... )\n",
      "                         time ticker   price  quantity     bid     ask\n",
      "    0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n",
      "    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n",
      "    2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n",
      "    3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n",
      "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.merge_asof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write/Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  1| 10|\n",
      "|  red|carrot|  3| 30|\n",
      "|  red|banana|  7| 70|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('getting_started.csv', header=True)\n",
    "spark.read.csv('getting_started.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a68e1e91807765992f7170bf312edb56fcc694697844342a16dd71da683d997"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pyspark3_09]",
   "language": "python",
   "name": "conda-env-pyspark3_09-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
